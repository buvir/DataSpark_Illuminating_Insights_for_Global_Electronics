{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***DataSpark: Illuminating Insights for Global Electronics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\.virtualenvs\\pythonproject\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting numpy>=1.22.4\n",
      "  Downloading numpy-2.2.4-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\.virtualenvs\\pythonproject\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: tzdata, pytz, numpy, pandas\n",
      "Successfully installed numpy-2.2.4 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\USER\\.virtualenvs\\pythonProject\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded with latin1 encoding!\n",
      "\n",
      "First 5 rows of df_Customers:\n",
      "   CustomerKey  Gender               Name            City State Code  \\\n",
      "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
      "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
      "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
      "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
      "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
      "\n",
      "               State Zip Code    Country  Continent    Birthday  \n",
      "0    South Australia     5523  Australia  Australia    7/3/1939  \n",
      "1  Western Australia     6522  Australia  Australia   9/27/1979  \n",
      "2           Victoria     3380  Australia  Australia   5/26/1947  \n",
      "3    South Australia     5223  Australia  Australia   9/17/1957  \n",
      "4           Victoria     3698  Australia  Australia  11/19/1965  \n",
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15256 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# loading customers.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:/Users/USER/Desktop/DataSpark_Illuminating_Insights_for_Global_Electronics/Customers.csv\"\n",
    "\n",
    "# Method 1: Try common encodings automatically\n",
    "encodings = ['utf-8', 'latin1', 'cp1252', 'ISO-8859-1']\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        df_Customers = pd.read_csv(file_path, encoding=encoding)\n",
    "        print(f\"Successfully loaded with {encoding} encoding!\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "\n",
    "# Method 2: Manual encoding specification (if automatic fails)\n",
    "# df_Customers = pd.read_csv(file_path, encoding='latin1')  # Most reliable fallback\n",
    "\n",
    "# Verify the DataFrame\n",
    "print(\"\\nFirst 5 rows of df_Customers:\")\n",
    "print(df_Customers.head())\n",
    "\n",
    "print(\"\\nDataFrame info:\")\n",
    "print(df_Customers.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerKey     int64\n",
       "Gender         object\n",
       "Name           object\n",
       "City           object\n",
       "State Code     object\n",
       "State          object\n",
       "Zip Code       object\n",
       "Country        object\n",
       "Continent      object\n",
       "Birthday       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Customers.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomerKey             int64\n",
      "Gender         string[python]\n",
      "Name           string[python]\n",
      "City           string[python]\n",
      "State Code     string[python]\n",
      "State          string[python]\n",
      "Zip Code       string[python]\n",
      "Country        string[python]\n",
      "Continent      string[python]\n",
      "Birthday       datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert categorical/text columns to string\n",
    "df_Customers[\"Gender\"] = df_Customers[\"Gender\"].astype(\"string\")\n",
    "df_Customers[\"Name\"] = df_Customers[\"Name\"].astype(\"string\")\n",
    "df_Customers[\"City\"] = df_Customers[\"City\"].astype(\"string\")\n",
    "df_Customers[\"State\"] = df_Customers[\"State\"].astype(\"string\")\n",
    "df_Customers[\"Country\"] = df_Customers[\"Country\"].astype(\"string\")\n",
    "df_Customers[\"Continent\"] = df_Customers[\"Continent\"].astype(\"string\")\n",
    "df_Customers[\"State Code\"] = df_Customers[\"State Code\"].astype(\"string\")\n",
    "df_Customers[\"Zip Code\"] = df_Customers[\"Zip Code\"].astype(\"string\")\n",
    "\n",
    "# Convert Birthday to datetime format\n",
    "df_Customers[\"Birthday\"] = pd.to_datetime(df_Customers[\"Birthday\"], format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "\n",
    "# Verify the changes\n",
    "print(df_Customers.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Customers_cleaned = df_Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomerKey     int64\n",
      "Gender         object\n",
      "Name           object\n",
      "City           object\n",
      "State Code     object\n",
      "State          object\n",
      "Zip Code       object\n",
      "Country        object\n",
      "Continent      object\n",
      "Birthday       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Verify the changes\n",
    "print(df_Customers_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Customers_cleaned.to_csv('df_Customers_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Customers_cleaned = pd.read_csv('df_Customers_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerKey</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State Code</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Country</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Birthday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>301</td>\n",
       "      <td>Female</td>\n",
       "      <td>Lilly Harding</td>\n",
       "      <td>WANDEARAH EAST</td>\n",
       "      <td>SA</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>5523</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>1939-07-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>325</td>\n",
       "      <td>Female</td>\n",
       "      <td>Madison Hull</td>\n",
       "      <td>MOUNT BUDD</td>\n",
       "      <td>WA</td>\n",
       "      <td>Western Australia</td>\n",
       "      <td>6522</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>1979-09-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>554</td>\n",
       "      <td>Female</td>\n",
       "      <td>Claire Ferres</td>\n",
       "      <td>WINJALLOK</td>\n",
       "      <td>VIC</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3380</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>1947-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>786</td>\n",
       "      <td>Male</td>\n",
       "      <td>Jai Poltpalingada</td>\n",
       "      <td>MIDDLE RIVER</td>\n",
       "      <td>SA</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>5223</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>1957-09-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1042</td>\n",
       "      <td>Male</td>\n",
       "      <td>Aidan Pankhurst</td>\n",
       "      <td>TAWONGA SOUTH</td>\n",
       "      <td>VIC</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>3698</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>1965-11-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerKey  Gender               Name            City State Code  \\\n",
       "0          301  Female      Lilly Harding  WANDEARAH EAST         SA   \n",
       "1          325  Female       Madison Hull      MOUNT BUDD         WA   \n",
       "2          554  Female      Claire Ferres       WINJALLOK        VIC   \n",
       "3          786    Male  Jai Poltpalingada    MIDDLE RIVER         SA   \n",
       "4         1042    Male    Aidan Pankhurst   TAWONGA SOUTH        VIC   \n",
       "\n",
       "               State Zip Code    Country  Continent    Birthday  \n",
       "0    South Australia     5523  Australia  Australia  1939-07-03  \n",
       "1  Western Australia     6522  Australia  Australia  1979-09-27  \n",
       "2           Victoria     3380  Australia  Australia  1947-05-26  \n",
       "3    South Australia     5223  Australia  Australia  1957-09-17  \n",
       "4           Victoria     3698  Australia  Australia  1965-11-19  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Customers_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Using cached psycopg2-2.9.10-cp310-cp310-win_amd64.whl (1.2 MB)\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\USER\\.virtualenvs\\pythonProject\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading sqlalchemy-2.0.40-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "Collecting greenlet>=1\n",
      "  Using cached greenlet-3.1.1-cp310-cp310-win_amd64.whl (298 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\user\\.virtualenvs\\pythonproject\\lib\\site-packages (from sqlalchemy) (4.13.0)\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.1.1 sqlalchemy-2.0.40\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\USER\\.virtualenvs\\pythonProject\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'customers' does not exist. Creating it now...\n",
      "Table created successfully.\n",
      "Data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "#push into pg sql , Customers_cleaned data _ final\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "try:\n",
    "    # Connect to PostgreSQL\n",
    "    connection = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\",\n",
    "        database=\"Global_Electronics_Data\",\n",
    "        user=\"postgres\",\n",
    "        password=\"sample12\"\n",
    "    )\n",
    "    connection.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Check if table exists\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT EXISTS (\n",
    "            SELECT FROM information_schema.tables \n",
    "            WHERE table_name = 'customers'\n",
    "        );\n",
    "    \"\"\")\n",
    "    table_exists = cursor.fetchone()[0]\n",
    "\n",
    "    if not table_exists:\n",
    "        print(\"Table 'customers' does not exist. Creating it now...\")\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE customers (\n",
    "                CustomerKey INT PRIMARY KEY,\n",
    "                Gender VARCHAR(255),\n",
    "                Name TEXT,\n",
    "                City VARCHAR(255),\n",
    "                State_Code VARCHAR(255),\n",
    "                State VARCHAR(255),\n",
    "                Zip_Code VARCHAR(255),  -- Changed from INT to VARCHAR\n",
    "                Country VARCHAR(255),\n",
    "                Continent VARCHAR(255),\n",
    "                Birthday DATE\n",
    "            );\n",
    "        \"\"\")\n",
    "        print(\"Table created successfully.\")\n",
    "    \n",
    "    # Insert data\n",
    "    for index, row in df_Customers_cleaned.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO customers (CustomerKey, Gender, Name, City, State_Code, \n",
    "            State, Zip_Code, Country, Continent, Birthday) \n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\", (\n",
    "            row['CustomerKey'], row['Gender'], row['Name'], row['City'],\n",
    "            row['State Code'], row['State'], str(row['Zip Code']),  # Ensure Zip Code is string\n",
    "            row['Country'], row['Continent'], row['Birthday']\n",
    "        ))\n",
    "\n",
    "    connection.commit()\n",
    "    print(\"Data inserted successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while interacting with the database: {e}\")\n",
    "\n",
    "finally:\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if connection:\n",
    "        connection.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   CustomerKey  15266 non-null  int64 \n",
      " 1   Gender       15266 non-null  object\n",
      " 2   Name         15266 non-null  object\n",
      " 3   City         15266 non-null  object\n",
      " 4   State Code   15256 non-null  object\n",
      " 5   State        15266 non-null  object\n",
      " 6   Zip Code     15266 non-null  object\n",
      " 7   Country      15266 non-null  object\n",
      " 8   Continent    15266 non-null  object\n",
      " 9   Birthday     15266 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_Customers_cleaned.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#push into pg sql , Customers_cleaned data\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "try:\n",
    "    # Connect to PostgreSQL\n",
    "    connection = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\",\n",
    "        database=\"Global_Electronics_Data\",\n",
    "        user=\"postgres\",\n",
    "        password=\"sample12\"\n",
    "    )\n",
    "    connection.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Check if table exists\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT EXISTS (\n",
    "            SELECT FROM information_schema.tables \n",
    "            WHERE table_name = 'Customers'\n",
    "        );\n",
    "    \"\"\")\n",
    "    table_exists = cursor.fetchone()[0]\n",
    "\n",
    "    if not table_exists:\n",
    "        print(\"Table 'bus_routes' does not exist. Creating it now...\")\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE bus_routes (\n",
    "                CustomerKey PRIMARY KEY,\n",
    "                Gender VARCHAR(255),\n",
    "                Name TEXT,\n",
    "                City VARCHAR(255),\n",
    "                State_Code VARCHAR(255),\n",
    "                State VARCHAR(255),\n",
    "                Zip_Code INT,\n",
    "                Country VARCHAR(255),\n",
    "                Continent VARCHAR(255),\n",
    "                Birthday ,\n",
    "            );\n",
    "        \"\"\")\n",
    "        print(\"Table created successfully.\")\n",
    "    \n",
    "\n",
    "    # Insert data\n",
    "    for index,row in df_Customers_cleaned.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO bus_routes (CustomerKey, Gender, Name,City, State_Code, \n",
    "            State, Zip_Code, Country, Continent,Birthday) \n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\", (\n",
    "            row['CustomerKey'], row['Gender'], row['Name'], row['City'],\n",
    "            row['State Code'], row['State'],row['Zip Code'],row['Country'],row['Continent'],row['Birthday']\n",
    "        ))\n",
    "\n",
    "    connection.commit()\n",
    "    print(\"Data inserted successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while interacting with the database: {e}\")\n",
    "\n",
    "finally:\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if connection:\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#push into pg sql , Customers_cleaned data\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "try:\n",
    "    # Connect to PostgreSQL\n",
    "    connection = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\",\n",
    "        database=\"red_bus\",\n",
    "        user=\"postgres\",\n",
    "        password=\"sample12\"\n",
    "    )\n",
    "    connection.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Check if table exists\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT EXISTS (\n",
    "            SELECT FROM information_schema.tables \n",
    "            WHERE table_name = 'bus_routes'\n",
    "        );\n",
    "    \"\"\")\n",
    "    table_exists = cursor.fetchone()[0]\n",
    "\n",
    "    if not table_exists:\n",
    "        print(\"Table 'bus_routes' does not exist. Creating it now...\")\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE bus_routes (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                route_name VARCHAR(255),\n",
    "                route_link TEXT,\n",
    "                bus_name VARCHAR(255),\n",
    "                bus_type VARCHAR(255),\n",
    "                departing_time TIME,\n",
    "                duration VARCHAR(50),\n",
    "                reaching_time TIME,\n",
    "                star_rating FLOAT,\n",
    "                price FLOAT,\n",
    "                seats_available INT\n",
    "            );\n",
    "        \"\"\")\n",
    "        print(\"Table created successfully.\")\n",
    "    else:\n",
    "        print(\"Table 'bus_routes' exists. Resetting sequence...\")\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT setval('bus_routes_id_seq', COALESCE((SELECT MAX(id) FROM bus_routes), 1), true);\n",
    "        \"\"\")\n",
    "\n",
    "    # Insert data\n",
    "    for index,row in bus_dataframe_all_bus_data_df_APSRTC_pvt.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO bus_routes (route_name, route_link, bus_name, bus_type, departing_time, duration, \n",
    "            reaching_time, star_rating, price, seats_available) \n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\", (\n",
    "            row['route_name'], row['route_link'], row['bus_name'], row['bus_type'],\n",
    "            datetime.strptime(row['departing_time'], \"%H:%M\").time() if row['departing_time'] else None,\n",
    "            row['duration'],\n",
    "            datetime.strptime(row['reaching_time'], \"%H:%M\").time() if row['reaching_time'] else None,\n",
    "            float(row['star_rating']) if row['star_rating'] else None,\n",
    "            float(row['price']) if row['price'] else None,\n",
    "            int(row['seats_available']) if row['seats_available'] else None\n",
    "        ))\n",
    "\n",
    "    connection.commit()\n",
    "    print(\"Data inserted successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while interacting with the database: {e}\")\n",
    "\n",
    "finally:\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if connection:\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'customers_cleaned.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load data, ensuring no extra index column\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df_Customers \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustomers_cleaned.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mZip Code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Drop the Unnamed column if it still appears\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#df_Customers = df_Customers.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# ✅ Correcting data types\u001b[39;00m\n\u001b[0;32m     10\u001b[0m df_Customers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomerKey\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_Customers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomerKey\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\.virtualenvs\\pythonProject\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\.virtualenvs\\pythonProject\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\USER\\.virtualenvs\\pythonProject\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\.virtualenvs\\pythonProject\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\USER\\.virtualenvs\\pythonProject\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'customers_cleaned.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data, ensuring no extra index column\n",
    "df_Customers = pd.read_csv(\"customers_cleaned.csv\", dtype={\"Zip Code\": \"string\"}, index_col=0)\n",
    "\n",
    "# Drop the Unnamed column if it still appears\n",
    "#df_Customers = df_Customers.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "# ✅ Correcting data types\n",
    "df_Customers[\"CustomerKey\"] = df_Customers[\"CustomerKey\"].astype(int)\n",
    "df_Customers[\"Gender\"] = df_Customers[\"Gender\"].astype(\"string\")\n",
    "df_Customers[\"Name\"] = df_Customers[\"Name\"].astype(\"string\")\n",
    "df_Customers[\"City\"] = df_Customers[\"City\"].astype(\"string\")\n",
    "df_Customers[\"State\"] = df_Customers[\"State\"].astype(\"string\")\n",
    "df_Customers[\"Zip Code\"] = df_Customers[\"Zip Code\"].astype(\"string\")\n",
    "df_Customers[\"Country\"] = df_Customers[\"Country\"].astype(\"string\")\n",
    "df_Customers[\"Continent\"] = df_Customers[\"Continent\"].astype(\"string\")\n",
    "\n",
    "# 🛠 Fix \"State Code\"\n",
    "df_Customers[\"State Code\"] = pd.to_numeric(df_Customers[\"State Code\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# ⏳ Convert \"Birthday\" to datetime\n",
    "df_Customers[\"Birthday\"] = pd.to_datetime(df_Customers[\"Birthday\"],format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "\n",
    "# # 🔥 Save the cleaned DataFrame correctly\n",
    "# df_Customers.to_csv(\"customers_cleaned.csv\", index=False)\n",
    "\n",
    "# ✅ Verify the changes\n",
    "print(df_Customers.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SA' 'WA' 'VIC' 'QLD' 'NT' 'NSW' 'TAS' 'ACT' 'BC' 'QC' 'ON' 'AB' 'NS'\n",
      " 'SK' 'NU' 'PE' 'MB' 'NL' 'YT' 'NB' 'BB' 'RP' 'BY' 'BW' 'NW' 'NI' 'ST'\n",
      " 'MV' 'SN' 'TH' 'BE' 'HE' 'SL' 'SH' 'HB' 'HH' 'RA' 'IL' 'GY' 'AL' 'AQ'\n",
      " 'CA' 'NP' 'GD' 'FC' 'PA' 'PC' 'LI' 'MP' 'PI' 'HN' 'AU' 'PL' 'LO' 'BN'\n",
      " 'CE' 'LN' 'MQ' 'BO' 'BR' 'CO' 'MY' 'RC' 'VI' 'FE' 'RM' 'AG' 'IM' 'MI'\n",
      " 'PR' 'BG' 'RG' 'PN' 'SV' 'LU' 'CN' 'TN' 'LE' 'PD' 'BI' 'CH' 'GE' 'TO'\n",
      " 'VV' 'CZ' 'AN' 'PG' 'LT' 'BL' 'TV' 'PV' 'MN' 'VA' 'PT' 'SI' 'MS' 'CT'\n",
      " 'BS' 'SS' 'RO' 'CR' 'FI' 'GR' 'IS' 'SO' 'VE' 'OR' 'ME' 'VR' 'CS' 'BZ'\n",
      " 'NO' 'AV' 'TA' 'VC' 'GO' 'MO' 'FR' 'FG' 'TE' 'BA' 'UD' 'AP' 'TP' 'RE'\n",
      " 'PO' nan 'AT' 'SR' 'RI' 'TS' 'KR' 'MT' 'PZ' 'MC' 'VT' 'AR' 'AO' 'CB' 'LC'\n",
      " 'SP' 'RN' 'FO' 'TR' 'UT' 'NH' 'DR' 'ZH' 'FL' 'OV' 'ZE' 'Falkirk'\n",
      " 'Ceredigion' 'North East Lincolnshire' 'Aberdeenshire' 'York'\n",
      " 'Pembrokeshire' 'Leicester' 'Highland' 'Tendring' 'Horsham' 'Newport'\n",
      " 'Bristol' 'Newark and Sherwood' 'Argyllshire' 'Lincoln' 'Tamworth'\n",
      " 'Fylde' 'Lewes' 'Rhondda Cynon Taf' 'Bromsgrove' 'Ripon' 'Cornwall'\n",
      " 'South Lanarkshire' 'Shropshire' 'Perth and Kinross' 'Crawley'\n",
      " 'Staffordshire' 'Mendip' 'Forest Heath' 'Moray' 'Bracknell Forest'\n",
      " 'Anglesey' 'Bolsover' 'Calderdale' 'Ashford' 'Sussex' 'Darlington'\n",
      " 'East Devon' 'Monmouthshire' 'Gloucester' 'Mid Devon' 'Somerset'\n",
      " 'Hereford' 'Bath and North East Somerset' 'Bassetlaw' 'Christchurch'\n",
      " 'West Berkshire' 'Gwynedd' 'Suffolk' 'Tewkesbury' 'Colchester'\n",
      " 'Llandrindod Wells' 'Harrogate' 'Winchester' 'Angus' 'Derbyshire Dales'\n",
      " 'Dacorum' 'Suffolk Coastal' 'Wiltshire' 'Leeds' 'Kennet' 'Hampshire'\n",
      " 'Norfolk' 'Northumberland' 'Doncaster' 'Purbeck' 'Wyre Forest'\n",
      " 'Redcar & Cleveland' 'West Dorset' 'Ipswich' 'Powys' 'Midlothian' 'Fife'\n",
      " 'North Ayrshire' 'Carmarthenshire' 'Birmingham' 'South Oxfordshire'\n",
      " 'North Yorkshire' 'Cheshire West and Chester' 'Scottish Borders'\n",
      " 'Lichfield' 'Rushcliffe' 'Arun' 'Dumfriesshire' 'Wakefield'\n",
      " 'Denbighshire' 'Plymouth' 'St Edmundsbury' 'Edinburgh' 'Liverpool' 'Kent'\n",
      " 'Warwick' 'Gedling' 'Shetland' 'Flintshire' 'Gravesham'\n",
      " 'Central Bedfordshire' 'North Dorset' 'Lancaster' 'Breckland'\n",
      " 'East Riding of Yorkshire' 'Exeter' 'Vale of White Horse' 'Cherwell'\n",
      " 'South Holland' 'South Lakeland' 'Stroud' 'Orkney Islands' 'West Lindsey'\n",
      " 'Stratford-on-Avon' 'West Oxfordshire' 'Bedford' 'Rother' 'Isle of Man'\n",
      " 'Swindon' 'Charnwood' 'Waverley' 'Wolverhampton' 'Aylesbury Vale'\n",
      " 'Merton' 'Sevenoaks' 'Allerdale' 'Bolton' 'Selby' 'Berkshire' 'Copeland'\n",
      " 'Chelmsford' 'South Kesteven' 'Cheshire East' 'Boston' 'County Durham'\n",
      " 'Mid Suffolk' 'Wandsworth' 'West Dunbartonshire' 'Erewash' 'Babergh'\n",
      " 'South Hams' 'Wigan' 'South Ayrshire' 'Rotherham' 'Isle of Wight'\n",
      " 'Cotswold' 'Worcester' 'Rutland' 'East Northamptonshire' 'Rugby'\n",
      " 'Stirling' 'North Kesteven' 'Comhairle nan Eilean Siar' 'Walsall'\n",
      " 'Knowsley' 'South Somerset' 'North Lincolnshire' 'South Norfolk'\n",
      " 'Aberdeen' 'Welwyn Hatfield' 'Kirkcudbrightshire' 'Carlisle' 'Hillingdon'\n",
      " 'West Norfolk' 'Kirklees' 'New Forest' 'Swansea' 'Craven' 'Camden'\n",
      " 'North Somerset' 'Teignbridge' 'Melton' 'Conwy' 'Enfield' 'Glasgow'\n",
      " 'Braintree' 'Chichester' 'North Hertfordshire' 'East Lothian'\n",
      " 'Renfrewshire' 'Eden' 'Uttlesford' 'Mid Sussex' 'Peterborough' 'Ashfield'\n",
      " 'Redbridge' 'Sheffield' 'Newcastle' 'Harlow' 'East Hampshire'\n",
      " 'Caerphilly' 'North Warwickshire' 'Brentwood' 'Tameside'\n",
      " 'Brighton and Hove' 'Rossendale' 'Swale' 'Sutton' 'Huntingdonshire'\n",
      " 'St Albans' 'Dudley' 'East Hertfordshire' 'Guildford' 'Cambridge'\n",
      " 'Woking' 'Daventry' 'Vale of Glamorgan' 'East Ayrshire' 'Bradford'\n",
      " 'South Buckinghamshire' 'Merthyr Tydfil' 'West Lothian' 'East Lindsey'\n",
      " 'Hambleton' 'Tower Hamlets' 'Test Valley' 'Dartford' 'Sunderland'\n",
      " 'Medway' 'Rochdale' 'Bury' 'Oxford' 'Bridgend' 'South Derbyshire'\n",
      " 'Milton Keynes' 'Southampton' 'Wokingham' 'Nottingham' 'Stevenage'\n",
      " 'Wirral' 'Dundee' 'Amber Valley' 'Harrow' 'East Dorset' 'Maidstone'\n",
      " 'Wigtownshire' 'Gateshead' 'Telford and Wrekin' 'Wellingborough'\n",
      " 'Runnymede' 'Wrexham' 'Cannock Chase' 'Kensington and Chelsea'\n",
      " 'South Gloucestershire' 'Wycombe' 'South Staffordshire' 'Sandwell'\n",
      " 'Bromley' 'Torridge' 'Burnley' 'Sefton' 'Ribble Valley' 'Chiltern'\n",
      " 'Hastings' 'East Staffordshire' 'Barnet' 'Reigate and Banstead'\n",
      " 'Basildon' 'Tandridge' 'Westminster' 'Warrington' 'West Lancashire'\n",
      " 'Preston' 'Canterbury' 'Wealden' 'Havering' 'West Devon' 'Islington'\n",
      " 'Wyre' 'Kinross-Shire' 'Waveney' 'North Lanarkshire' 'Ely'\n",
      " 'Neath Port Talbot' 'Nuneaton & Bedworth' 'Chesterfield' 'Mole Valley'\n",
      " 'South Northamptonshire' 'Broxbourne' 'Cardiff' 'Hackney' 'Redditch'\n",
      " 'Tunbridge Wells' 'Haringey' 'Malvern Hills' 'Broxtowe' 'Spelthorne'\n",
      " 'Coventry' 'Newmarket' 'Lanarkshire' 'East Dunbartonshire'\n",
      " 'Stockton-on-Tees' 'NC' 'MD' 'HI' 'NY' 'TX' 'GA' 'DE' 'WI' 'KS' 'NV' 'MA'\n",
      " 'NJ' 'OH' 'IA' 'KY' 'DC' 'OK' 'ID' 'AZ' 'IN' 'WV' 'NE' 'LA' 'AK' 'ND'\n",
      " 'SC' 'NM' 'SD' 'WY']\n"
     ]
    }
   ],
   "source": [
    "print(df_Customers[\"State Code\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         5523\n",
      "1         6522\n",
      "2         3380\n",
      "3         5223\n",
      "4         3698\n",
      "         ...  \n",
      "15261    77017\n",
      "15262    22101\n",
      "15263    28405\n",
      "15264    92501\n",
      "15265    48302\n",
      "Name: Zip Code, Length: 15266, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_Customers[\"Zip Code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomerKey             int64\n",
      "Gender         string[python]\n",
      "Name           string[python]\n",
      "City           string[python]\n",
      "State Code     string[python]\n",
      "State          string[python]\n",
      "Zip Code       string[python]\n",
      "Country        string[python]\n",
      "Continent      string[python]\n",
      "Birthday       datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert categorical/text columns to string\n",
    "df_Customers[\"Gender\"] = df_Customers[\"Gender\"].astype(\"string\")\n",
    "df_Customers[\"Name\"] = df_Customers[\"Name\"].astype(\"string\")\n",
    "df_Customers[\"City\"] = df_Customers[\"City\"].astype(\"string\")\n",
    "df_Customers[\"State\"] = df_Customers[\"State\"].astype(\"string\")\n",
    "df_Customers[\"Country\"] = df_Customers[\"Country\"].astype(\"string\")\n",
    "df_Customers[\"Continent\"] = df_Customers[\"Continent\"].astype(\"string\")\n",
    "df_Customers[\"State Code\"] = df_Customers[\"State Code\"].astype(\"string\")\n",
    "df_Customers[\"Zip Code\"] = df_Customers[\"Zip Code\"].astype(\"string\")\n",
    "\n",
    "# Convert Birthday to datetime format\n",
    "df_Customers[\"Birthday\"] = pd.to_datetime(df_Customers[\"Birthday\"], format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "\n",
    "# Verify the changes\n",
    "print(df_Customers.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomerKey             int64\n",
      "Gender         string[python]\n",
      "Name           string[python]\n",
      "City           string[python]\n",
      "State Code     string[python]\n",
      "State          string[python]\n",
      "Zip Code       string[python]\n",
      "Country        string[python]\n",
      "Continent      string[python]\n",
      "Birthday       datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_Customers.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   CustomerKey  15266 non-null  int64         \n",
      " 1   Gender       15266 non-null  string        \n",
      " 2   Name         15266 non-null  string        \n",
      " 3   City         15266 non-null  string        \n",
      " 4   State Code   15256 non-null  string        \n",
      " 5   State        15266 non-null  string        \n",
      " 6   Zip Code     15266 non-null  string        \n",
      " 7   Country      15266 non-null  string        \n",
      " 8   Continent    15266 non-null  string        \n",
      " 9   Birthday     15266 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(1), string(8)\n",
      "memory usage: 1.2 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_Customers.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<StringArray>\n",
      "[ 'SA',  'WA', 'VIC', 'QLD',  'NT', 'NSW', 'TAS', 'ACT',  'BC',  'QC',\n",
      " ...\n",
      "  'IN',  'WV',  'NE',  'LA',  'AK',  'ND',  'SC',  'NM',  'SD',  'WY']\n",
      "Length: 468, dtype: string\n"
     ]
    }
   ],
   "source": [
    "print(df_Customers[\"State Code\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Customers.to_csv(\"customers_cleaned.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  CustomerKey  Gender               Name  \\\n",
      "0               0          301  Female      Lilly Harding   \n",
      "1               1          325  Female       Madison Hull   \n",
      "2               2          554  Female      Claire Ferres   \n",
      "3               3          786    Male  Jai Poltpalingada   \n",
      "4               4         1042    Male    Aidan Pankhurst   \n",
      "...           ...          ...     ...                ...   \n",
      "15261       15261      2099600  Female     Denisa Duková   \n",
      "15262       15262      2099618    Male   Justin Solórzano   \n",
      "15263       15263      2099758    Male    Svend Petrussen   \n",
      "15264       15264      2099862  Female       Lorenza Rush   \n",
      "15265       15265      2099937    Male   Zygmunt Kaminski   \n",
      "\n",
      "                      City State Code              State Zip Code  \\\n",
      "0           WANDEARAH EAST         SA    South Australia     5523   \n",
      "1               MOUNT BUDD         WA  Western Australia     6522   \n",
      "2                WINJALLOK        VIC           Victoria     3380   \n",
      "3             MIDDLE RIVER         SA    South Australia     5223   \n",
      "4            TAWONGA SOUTH        VIC           Victoria     3698   \n",
      "...                    ...        ...                ...      ...   \n",
      "15261              Houston         TX              Texas    77017   \n",
      "15262               Mclean         VA           Virginia    22101   \n",
      "15263           Wilmington         NC     North Carolina    28405   \n",
      "15264            Riverside         CA         California    92501   \n",
      "15265  Bloomfield Township         MI           Michigan    48302   \n",
      "\n",
      "             Country      Continent    Birthday  \n",
      "0          Australia      Australia  1939-07-03  \n",
      "1          Australia      Australia  1979-09-27  \n",
      "2          Australia      Australia  1947-05-26  \n",
      "3          Australia      Australia  1957-09-17  \n",
      "4          Australia      Australia  1965-11-19  \n",
      "...              ...            ...         ...  \n",
      "15261  United States  North America  1936-03-25  \n",
      "15262  United States  North America  1992-02-16  \n",
      "15263  United States  North America  1937-11-09  \n",
      "15264  United States  North America  1937-10-12  \n",
      "15265  United States  North America  1965-08-18  \n",
      "\n",
      "[15266 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned CSV\n",
    "df_customers_cleaned = pd.read_csv(\"customers_cleaned.csv\")\n",
    "\n",
    "# Display the entire DataFrame (if it's small)\n",
    "print(df_customers_cleaned)  # Shows all rows without truncation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15266 entries, 0 to 15265\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Unnamed: 0   15266 non-null  int64 \n",
      " 1   CustomerKey  15266 non-null  int64 \n",
      " 2   Gender       15266 non-null  object\n",
      " 3   Name         15266 non-null  object\n",
      " 4   City         15266 non-null  object\n",
      " 5   State Code   15256 non-null  object\n",
      " 6   State        15266 non-null  object\n",
      " 7   Zip Code     15266 non-null  object\n",
      " 8   Country      15266 non-null  object\n",
      " 9   Continent    15266 non-null  object\n",
      " 10  Birthday     15266 non-null  object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_customers_cleaned.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded with utf-8 encoding!\n",
      "\n",
      "First 5 rows of df_Products:\n",
      "   ProductKey                         Product Name    Brand   Color  \\\n",
      "0           1  Contoso 512MB MP3 Player E51 Silver  Contoso  Silver   \n",
      "1           2    Contoso 512MB MP3 Player E51 Blue  Contoso    Blue   \n",
      "2           3     Contoso 1G MP3 Player E100 White  Contoso   White   \n",
      "3           4    Contoso 2G MP3 Player E200 Silver  Contoso  Silver   \n",
      "4           5       Contoso 2G MP3 Player E200 Red  Contoso     Red   \n",
      "\n",
      "  Unit Cost USD Unit Price USD  SubcategoryKey Subcategory  CategoryKey  \\\n",
      "0        $6.62         $12.99              101     MP4&MP3            1   \n",
      "1        $6.62         $12.99              101     MP4&MP3            1   \n",
      "2        $7.40         $14.52              101     MP4&MP3            1   \n",
      "3       $11.00         $21.57              101     MP4&MP3            1   \n",
      "4       $11.00         $21.57              101     MP4&MP3            1   \n",
      "\n",
      "  Category  \n",
      "0    Audio  \n",
      "1    Audio  \n",
      "2    Audio  \n",
      "3    Audio  \n",
      "4    Audio  \n",
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2517 entries, 0 to 2516\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   ProductKey      2517 non-null   int64 \n",
      " 1   Product Name    2517 non-null   object\n",
      " 2   Brand           2517 non-null   object\n",
      " 3   Color           2517 non-null   object\n",
      " 4   Unit Cost USD   2517 non-null   object\n",
      " 5   Unit Price USD  2517 non-null   object\n",
      " 6   SubcategoryKey  2517 non-null   int64 \n",
      " 7   Subcategory     2517 non-null   object\n",
      " 8   CategoryKey     2517 non-null   int64 \n",
      " 9   Category        2517 non-null   object\n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 196.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# loading Products.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:/Users/USER/Desktop/DataSpark_Illuminating_Insights_for_Global_Electronics/Products.csv\"\n",
    "\n",
    "# Method 1: Try common encodings automatically\n",
    "encodings = ['utf-8', 'latin1', 'cp1252', 'ISO-8859-1']\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        df_Products = pd.read_csv(file_path, encoding=encoding)\n",
    "        print(f\"Successfully loaded with {encoding} encoding!\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "\n",
    "# Method 2: Manual encoding specification (if automatic fails)\n",
    "# df_Products = pd.read_csv(file_path, encoding='latin1')  # Most reliable fallback\n",
    "\n",
    "# Verify the DataFrame\n",
    "print(\"\\nFirst 5 rows of df_Products:\")\n",
    "print(df_Products.head())\n",
    "\n",
    "print(\"\\nDataFrame info:\")\n",
    "print(df_Products.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductKey         int64\n",
       "Product Name      object\n",
       "Brand             object\n",
       "Color             object\n",
       "Unit Cost USD     object\n",
       "Unit Price USD    object\n",
       "SubcategoryKey     int64\n",
       "Subcategory       object\n",
       "CategoryKey        int64\n",
       "Category          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Products.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Cost USD     object\n",
      "Unit Price USD    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_Products[['Unit Cost USD', 'Unit Price USD']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Products[\"Unit Cost USD\"] = df_Products[\"Unit Cost USD\"].replace('[\\$,]', '', regex=True).astype(float)\n",
    "df_Products[\"Unit Price USD\"] = df_Products[\"Unit Price USD\"].replace('[\\$,]', '', regex=True).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Cost USD     float64\n",
      "Unit Price USD    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_Products[['Unit Cost USD', 'Unit Price USD']].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ProductKey                         Product Name    Brand   Color  \\\n",
      "0           1  Contoso 512MB MP3 Player E51 Silver  Contoso  Silver   \n",
      "1           2    Contoso 512MB MP3 Player E51 Blue  Contoso    Blue   \n",
      "2           3     Contoso 1G MP3 Player E100 White  Contoso   White   \n",
      "3           4    Contoso 2G MP3 Player E200 Silver  Contoso  Silver   \n",
      "4           5       Contoso 2G MP3 Player E200 Red  Contoso     Red   \n",
      "\n",
      "   Unit Cost USD  Unit Price USD  SubcategoryKey Subcategory  CategoryKey  \\\n",
      "0           6.62           12.99             101     MP4&MP3            1   \n",
      "1           6.62           12.99             101     MP4&MP3            1   \n",
      "2           7.40           14.52             101     MP4&MP3            1   \n",
      "3          11.00           21.57             101     MP4&MP3            1   \n",
      "4          11.00           21.57             101     MP4&MP3            1   \n",
      "\n",
      "  Category  \n",
      "0    Audio  \n",
      "1    Audio  \n",
      "2    Audio  \n",
      "3    Audio  \n",
      "4    Audio  \n"
     ]
    }
   ],
   "source": [
    "print(df_Products.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '${:,.2f}'.format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ProductKey                         Product Name    Brand   Color  \\\n",
      "0           1  Contoso 512MB MP3 Player E51 Silver  Contoso  Silver   \n",
      "1           2    Contoso 512MB MP3 Player E51 Blue  Contoso    Blue   \n",
      "2           3     Contoso 1G MP3 Player E100 White  Contoso   White   \n",
      "3           4    Contoso 2G MP3 Player E200 Silver  Contoso  Silver   \n",
      "4           5       Contoso 2G MP3 Player E200 Red  Contoso     Red   \n",
      "\n",
      "   Unit Cost USD  Unit Price USD  SubcategoryKey Subcategory  CategoryKey  \\\n",
      "0          $6.62          $12.99             101     MP4&MP3            1   \n",
      "1          $6.62          $12.99             101     MP4&MP3            1   \n",
      "2          $7.40          $14.52             101     MP4&MP3            1   \n",
      "3         $11.00          $21.57             101     MP4&MP3            1   \n",
      "4         $11.00          $21.57             101     MP4&MP3            1   \n",
      "\n",
      "  Category  \n",
      "0    Audio  \n",
      "1    Audio  \n",
      "2    Audio  \n",
      "3    Audio  \n",
      "4    Audio  \n"
     ]
    }
   ],
   "source": [
    "print(df_Products.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ProductKey                         Product Name    Brand   Color  \\\n",
      "0           1  Contoso 512MB MP3 Player E51 Silver  Contoso  Silver   \n",
      "1           2    Contoso 512MB MP3 Player E51 Blue  Contoso    Blue   \n",
      "2           3     Contoso 1G MP3 Player E100 White  Contoso   White   \n",
      "3           4    Contoso 2G MP3 Player E200 Silver  Contoso  Silver   \n",
      "4           5       Contoso 2G MP3 Player E200 Red  Contoso     Red   \n",
      "\n",
      "   Unit Cost USD  Unit Price USD  SubcategoryKey Subcategory  CategoryKey  \\\n",
      "0          $6.62          $12.99             101     MP4&MP3            1   \n",
      "1          $6.62          $12.99             101     MP4&MP3            1   \n",
      "2          $7.40          $14.52             101     MP4&MP3            1   \n",
      "3         $11.00          $21.57             101     MP4&MP3            1   \n",
      "4         $11.00          $21.57             101     MP4&MP3            1   \n",
      "\n",
      "  Category  \n",
      "0    Audio  \n",
      "1    Audio  \n",
      "2    Audio  \n",
      "3    Audio  \n",
      "4    Audio  \n"
     ]
    }
   ],
   "source": [
    "print(df_Products.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Products[\"Unit Cost USD\"] = df_Products[\"Unit Cost USD\"].apply(lambda x: f'${x:,.2f}')\n",
    "df_Products[\"Unit Price USD\"] = df_Products[\"Unit Price USD\"].apply(lambda x: f'${x:,.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ProductKey                         Product Name    Brand   Color  \\\n",
      "0           1  Contoso 512MB MP3 Player E51 Silver  Contoso  Silver   \n",
      "1           2    Contoso 512MB MP3 Player E51 Blue  Contoso    Blue   \n",
      "2           3     Contoso 1G MP3 Player E100 White  Contoso   White   \n",
      "3           4    Contoso 2G MP3 Player E200 Silver  Contoso  Silver   \n",
      "4           5       Contoso 2G MP3 Player E200 Red  Contoso     Red   \n",
      "\n",
      "  Unit Cost USD Unit Price USD  SubcategoryKey Subcategory  CategoryKey  \\\n",
      "0         $6.62         $12.99             101     MP4&MP3            1   \n",
      "1         $6.62         $12.99             101     MP4&MP3            1   \n",
      "2         $7.40         $14.52             101     MP4&MP3            1   \n",
      "3        $11.00         $21.57             101     MP4&MP3            1   \n",
      "4        $11.00         $21.57             101     MP4&MP3            1   \n",
      "\n",
      "  Category  \n",
      "0    Audio  \n",
      "1    Audio  \n",
      "2    Audio  \n",
      "3    Audio  \n",
      "4    Audio  \n"
     ]
    }
   ],
   "source": [
    "print(df_Products.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Cost USD     object\n",
      "Unit Price USD    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_Products[['Unit Cost USD', 'Unit Price USD']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded with utf-8 encoding!\n",
      "\n",
      "First 5 rows of df_Stores:\n",
      "   StoreKey    Country                         State  Square Meters  Open Date\n",
      "0         1  Australia  Australian Capital Territory          595.0   1/1/2008\n",
      "1         2  Australia            Northern Territory          665.0  1/12/2008\n",
      "2         3  Australia               South Australia         2000.0   1/7/2012\n",
      "3         4  Australia                      Tasmania         2000.0   1/1/2010\n",
      "4         5  Australia                      Victoria         2000.0  12/9/2015\n",
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 67 entries, 0 to 66\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   StoreKey       67 non-null     int64  \n",
      " 1   Country        67 non-null     object \n",
      " 2   State          67 non-null     object \n",
      " 3   Square Meters  66 non-null     float64\n",
      " 4   Open Date      67 non-null     object \n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 2.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# loading Stores.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:/Users/USER/Desktop/DataSpark_Illuminating_Insights_for_Global_Electronics/Stores.csv\"\n",
    "\n",
    "# Method 1: Try common encodings automatically\n",
    "encodings = ['utf-8', 'latin1', 'cp1252', 'ISO-8859-1']\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        df_Stores = pd.read_csv(file_path, encoding=encoding)\n",
    "        print(f\"Successfully loaded with {encoding} encoding!\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "\n",
    "# Method 2: Manual encoding specification (if automatic fails)\n",
    "# df_Stores = pd.read_csv(file_path, encoding='latin1')  # Most reliable fallback\n",
    "\n",
    "# Verify the DataFrame\n",
    "print(\"\\nFirst 5 rows of df_Stores:\")\n",
    "print(df_Stores.head())\n",
    "\n",
    "print(\"\\nDataFrame info:\")\n",
    "print(df_Stores.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded with utf-8 encoding!\n",
      "\n",
      "First 5 rows of Exchange_Rates:\n",
      "       Date Currency  Exchange\n",
      "0  1/1/2015      USD    1.0000\n",
      "1  1/1/2015      CAD    1.1583\n",
      "2  1/1/2015      AUD    1.2214\n",
      "3  1/1/2015      EUR    0.8237\n",
      "4  1/1/2015      GBP    0.6415\n",
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11215 entries, 0 to 11214\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Date      11215 non-null  object \n",
      " 1   Currency  11215 non-null  object \n",
      " 2   Exchange  11215 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 263.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# loading Exchange_Rates.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:/Users/USER/Desktop/DataSpark_Illuminating_Insights_for_Global_Electronics/Exchange_Rates.csv\"\n",
    "\n",
    "# Method 1: Try common encodings automatically\n",
    "encodings = ['utf-8', 'latin1', 'cp1252', 'ISO-8859-1']\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        df_Exchange_Rates = pd.read_csv(file_path, encoding=encoding)\n",
    "        print(f\"Successfully loaded with {encoding} encoding!\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "\n",
    "# Method 2: Manual encoding specification (if automatic fails)\n",
    "# df_Stores = pd.readf_Exchange_Ratesd_csv(file_path, encoding='latin1')  # Most reliable fallback\n",
    "\n",
    "# Verify the DataFrame\n",
    "print(\"\\nFirst 5 rows of Exchange_Rates:\")\n",
    "print(df_Exchange_Rates.head())\n",
    "\n",
    "print(\"\\nDataFrame info:\")\n",
    "print(df_Exchange_Rates.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded with utf-8 encoding!\n",
      "\n",
      "First 5 rows of df_Sales:\n",
      "   Order Number  Line Item Order Date Delivery Date  CustomerKey  StoreKey  \\\n",
      "0        366000          1   1/1/2016           NaN       265598        10   \n",
      "1        366001          1   1/1/2016     1/13/2016      1269051         0   \n",
      "2        366001          2   1/1/2016     1/13/2016      1269051         0   \n",
      "3        366002          1   1/1/2016     1/12/2016       266019         0   \n",
      "4        366002          2   1/1/2016     1/12/2016       266019         0   \n",
      "\n",
      "   ProductKey  Quantity Currency Code  \n",
      "0        1304         1           CAD  \n",
      "1        1048         2           USD  \n",
      "2        2007         1           USD  \n",
      "3        1106         7           CAD  \n",
      "4         373         1           CAD  \n",
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62884 entries, 0 to 62883\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Order Number   62884 non-null  int64 \n",
      " 1   Line Item      62884 non-null  int64 \n",
      " 2   Order Date     62884 non-null  object\n",
      " 3   Delivery Date  13165 non-null  object\n",
      " 4   CustomerKey    62884 non-null  int64 \n",
      " 5   StoreKey       62884 non-null  int64 \n",
      " 6   ProductKey     62884 non-null  int64 \n",
      " 7   Quantity       62884 non-null  int64 \n",
      " 8   Currency Code  62884 non-null  object\n",
      "dtypes: int64(6), object(3)\n",
      "memory usage: 4.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# loading Sales.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path = r\"C:/Users/USER/Desktop/DataSpark_Illuminating_Insights_for_Global_Electronics/Sales.csv\"\n",
    "\n",
    "# Method 1: Try common encodings automatically\n",
    "encodings = ['utf-8', 'latin1', 'cp1252', 'ISO-8859-1']\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        df_Sales = pd.read_csv(file_path, encoding=encoding)\n",
    "        print(f\"Successfully loaded with {encoding} encoding!\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "\n",
    "# Method 2: Manual encoding specification (if automatic fails)\n",
    "# df_Products = pd.read_csv(file_path, encoding='latin1')  # Most reliable fallback\n",
    "\n",
    "# Verify the DataFrame\n",
    "print(\"\\nFirst 5 rows of df_Sales:\")\n",
    "print(df_Sales.head())\n",
    "\n",
    "print(\"\\nDataFrame info:\")\n",
    "print(df_Sales.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
